{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## necesssary dependency to install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Descriptive Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic statistics for textual lengths\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load data from a file (accepting the path)\n",
    "file_path = \"../assets/data/raw_analyst_ratings.csv\"  # Replace 'your_dataset.csv' with the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset has a 'headline' column\n",
    "if 'headline' not in df.columns:\n",
    "    raise ValueError(\"The dataset must contain a 'headline' column.\")\n",
    "\n",
    "# Calculate headline lengths (in characters and words)\n",
    "df['length_characters'] = df['headline'].str.len()\n",
    "df['length_words'] = df['headline'].str.split().apply(len)\n",
    "\n",
    "# Basic statistics\n",
    "stats = {\n",
    "    'Total Headlines': len(df),\n",
    "    'Average Length (Characters)': df['length_characters'].mean(),\n",
    "    # 'Median Length (Characters)': df['length_characters'].median(),\n",
    "    # 'Standard Deviation (Characters)': df['length_characters'].std(),\n",
    "    'Average Length (Words)': df['length_words'].mean(),\n",
    "    # 'Median Length (Words)': df['length_words'].median(),\n",
    "    # 'Standard Deviation (Words)': df['length_words'].std(),\n",
    "    'Longest Headline (Characters)': df['length_characters'].max(),\n",
    "    'Shortest Headline (Characters)': df['length_characters'].min(),\n",
    "    'Longest Headline (Words)': df['length_words'].max(),\n",
    "    'Shortest Headline (Words)': df['length_words'].min(),\n",
    "}\n",
    "\n",
    "# Print the DataFrame and statistics\n",
    "print(\"Headline Data:\")\n",
    "print(df.head())  # Display the first few rows of the dataset\n",
    "print(\"\\nBasic Statistics:\")\n",
    "for stat, value in stats.items():\n",
    "    print(f\"{stat}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"../assets/data/raw_analyst_ratings.csv\"  # Replace 'your_dataset.csv' with the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "# Count articles per publisher\n",
    "publisher_counts = df['publisher'].value_counts()\n",
    "\n",
    "# Print articles per publisher\n",
    "print(\"Articles per Publisher:\")\n",
    "print(publisher_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze publication dates for trends over time:\n",
    "if 'date' not in df.columns:\n",
    "    raise ValueError(\"The dataset must contain a 'date' column.\")\n",
    "df['date'] = pd.to_datetime(df['date'], format=\"%Y-%m-%d %H:%M:%S\", errors='coerce')\n",
    "\n",
    "# Check for invalid dates\n",
    "if df['date'].isnull().any():\n",
    "    print(\"Warning: Some publication dates could not be parsed and were set to NaT.\")\n",
    "# Extract day of the week and year-month for analysis\n",
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "df['year_month'] = df['date'].dt.to_period('M')\n",
    "\n",
    "# Count articles by day of the week\n",
    "articles_by_day = df['day_of_week'].value_counts()\n",
    "\n",
    "# Count articles by year-month\n",
    "articles_by_month = df['year_month'].value_counts().sort_index()\n",
    "\n",
    "# Print the trends\n",
    "print(\"Articles by Day of the Week:\")\n",
    "print(articles_by_day)\n",
    "print(\"\\nArticles by Year-Month:\")\n",
    "print(articles_by_month)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis(Sentiment analysis & Topic Modeling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a CSV file\n",
    "file_path = \"../assets/data/raw_analyst_ratings.csv\"  # Replace with the actual path to your file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset has a 'headline' column\n",
    "if 'headline' not in df.columns:\n",
    "    raise ValueError(\"The dataset must contain a 'headline' column.\")\n",
    "\n",
    "# Perform sentiment analysis\n",
    "def analyze_sentiment(headline):\n",
    "    analysis = TextBlob(headline)\n",
    "    polarity = analysis.polarity  # Sentiment polarity: -1 (negative) to +1 (positive)\n",
    "    if polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Apply the sentiment analysis function to the headlines\n",
    "df['sentiment'] = df['headline'].apply(analyze_sentiment)\n",
    "\n",
    "# Count the number of headlines by sentiment\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "\n",
    "# Print results\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "print(sentiment_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Load spaCy model for Named Entity Recognition\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"../assets/data/raw_analyst_ratings.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset has a 'headline' column\n",
    "if 'headline' not in df.columns:\n",
    "    raise ValueError(\"The dataset must contain a 'headline' column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text (lowercasing, removing punctuation, etc.)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Preprocess all headlines in parallel\n",
    "df['cleaned_headline'] = Parallel(n_jobs=-1)(delayed(preprocess_text)(text) for text in df['headline'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=20, lowercase=True)  # Top 20 keywords\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_headline'])\n",
    "\n",
    "# Get the top keywords based on TF-IDF scores\n",
    "keywords = tfidf_vectorizer.get_feature_names_out()\n",
    "print(\"Top Keywords from TF-IDF:\")\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "entity_results = []\n",
    "noun_phrase_results = []\n",
    "\n",
    "# Batch process headlines using spaCy's nlp.pipe\n",
    "for doc in nlp.pipe(df['headline'], batch_size=50):  # Adjust batch_size for performance\n",
    "    # Extract relevant entities\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'MONEY', 'DATE', 'PRODUCT']]\n",
    "    entity_results.append(entities)\n",
    "    \n",
    "    # Extract noun phrases\n",
    "    noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "    noun_phrase_results.append(noun_phrases)\n",
    "\n",
    "# Add extracted entities and noun phrases back to the dataframe\n",
    "df['entities'] = entity_results\n",
    "df['noun_phrases'] = noun_phrase_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all entities to get the most common ones\n",
    "all_entities = [entity for entities in df['entities'] for entity in entities]\n",
    "entity_counts = Counter(all_entities)\n",
    "\n",
    "print(\"\\nMost Common Entities:\")\n",
    "for entity, count in entity_counts.most_common(10):  # Top 10 most common entities\n",
    "    print(f\"{entity}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all noun phrases to get the most common ones\n",
    "all_phrases = [phrase for phrases in df['noun_phrases'] for phrase in phrases]\n",
    "phrase_counts = Counter(all_phrases)\n",
    "\n",
    "print(\"\\nMost Common Noun Phrases:\")\n",
    "for phrase, count in phrase_counts.most_common(10):  # Top 10 most common noun phrases\n",
    "    print(f\"{phrase}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the publication frequency vary over time? \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"../assets/data/raw_analyst_ratings.csv\"  # Replace with the actual path to your file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset has a 'publication_date' column\n",
    "if 'date' not in df.columns:\n",
    "    raise ValueError(\"The dataset must contain a 'date' column.\")\n",
    "\n",
    "# Convert the 'publication_date' column to datetime format (adjust the column name if necessary)\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid or missing publication dates\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Extract year, month, and day for time-based analysis\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "\n",
    "# Step 1: Count the number of publications per month (or any time period you want)\n",
    "monthly_publications = df.groupby(['year', 'month']).size()\n",
    "\n",
    "# Plot the number of publications per month\n",
    "plt.figure(figsize=(10, 6))\n",
    "monthly_publications.plot(kind='line', marker='o', color='b')\n",
    "plt.title('Monthly Publication Frequency')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Articles Published')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Identify spikes in publication frequency\n",
    "# Find the months with the largest change in publication frequency\n",
    "monthly_diff = monthly_publications.diff().abs()  # Difference in number of publications\n",
    "spikes = monthly_diff[monthly_diff > monthly_diff.quantile(0.95)]  # Top 5% of spikes\n",
    "\n",
    "print(\"\\nPotential Spikes in Publication Frequency:\")\n",
    "print(spikes)\n",
    "\n",
    "# If you want to analyze spikes during specific market events, you could cross-reference the spikes with event dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of publishing times might reveal if there’s a specific time when most news is released, \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"../assets/data/raw_analyst_ratings.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset has a 'publication_date' column\n",
    "if 'date' not in df.columns:\n",
    "    raise ValueError(\"The dataset must contain a 'date' column.\")\n",
    "\n",
    "# Convert the 'publication_date' column to datetime format (adjust the column name if necessary)\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid or missing publication dates\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Step 1: Extract the hour of publication\n",
    "df['hour'] = df['date'].dt.hour\n",
    "\n",
    "# Step 2: Count the number of publications per hour\n",
    "hourly_publications = df.groupby('hour').size()\n",
    "\n",
    "# Step 3: Visualize the distribution of publications by hour\n",
    "plt.figure(figsize=(10, 6))\n",
    "hourly_publications.plot(kind='bar', color='b')\n",
    "plt.title('Publication Frequency by Hour of Day')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Articles Published')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Identify peaks in the publication frequency\n",
    "peaks = hourly_publications[hourly_publications == hourly_publications.max()]\n",
    "\n",
    "print(\"\\nPeak Publication Hours:\")\n",
    "print(peaks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publisher Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which publishers contribute most to the news feed? Is there a difference in the type of news they report?\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"../assets/data/raw_analyst_ratings.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset has the necessary columns: 'publisher' and 'headline'\n",
    "if 'publisher' not in df.columns or 'headline' not in df.columns:\n",
    "    raise ValueError(\"The dataset must contain 'publisher' and 'headline' columns.\")\n",
    "\n",
    "# Step 1: Count articles by publisher\n",
    "publisher_counts = df['publisher'].value_counts()\n",
    "\n",
    "# Plot the number of articles per publisher\n",
    "plt.figure(figsize=(12, 6))\n",
    "publisher_counts.head(10).plot(kind='bar', color='b')  # Top 10 publishers\n",
    "plt.title('Top 10 Publishers by Number of Articles')\n",
    "plt.xlabel('Publisher')\n",
    "plt.ylabel('Number of Articles Published')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Sentiment Analysis by Publisher\n",
    "def analyze_sentiment(headline):\n",
    "    analysis = TextBlob(headline)\n",
    "    polarity = analysis.polarity  # Sentiment polarity: -1 (negative) to +1 (positive)\n",
    "    if polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Apply sentiment analysis to the headlines\n",
    "df['sentiment'] = df['headline'].apply(analyze_sentiment)\n",
    "\n",
    "# Group by publisher and get sentiment counts\n",
    "sentiment_by_publisher = df.groupby(['publisher', 'sentiment']).size().unstack().fillna(0)\n",
    "\n",
    "# Plot sentiment distribution by publisher\n",
    "sentiment_by_publisher.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "plt.title('Sentiment Distribution by Publisher')\n",
    "plt.xlabel('Publisher')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Keyword Extraction by Publisher (using TF-IDF)\n",
    "def get_top_keywords_by_publisher(df, publisher_name, top_n=10):\n",
    "    publisher_data = df[df['publisher'] == publisher_name]\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=top_n)\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(publisher_data['headline'])\n",
    "    return tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get top 10 keywords for the top publisher\n",
    "top_publisher = publisher_counts.index[0]\n",
    "top_keywords = get_top_keywords_by_publisher(df, top_publisher)\n",
    "\n",
    "print(f\"Top 10 keywords for {top_publisher}:\")\n",
    "print(top_keywords)\n",
    "\n",
    "# Step 4: Analyze topic distribution (e.g., using TF-IDF or Named Entity Recognition)\n",
    "# You can follow a similar method to extract entities or noun phrases for more advanced topic analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If email addresses are used as publisher names, identify unique domains to see if certain organizations contribute more frequently.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"../assets/data/raw_analyst_ratings.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset has the necessary 'publisher' column\n",
    "if 'publisher' not in df.columns:\n",
    "    raise ValueError(\"The dataset must contain a 'publisher' column.\")\n",
    "\n",
    "# Step 1: Extract domain from email addresses\n",
    "def extract_domain(email):\n",
    "    # Use regex to extract the domain part after '@'\n",
    "    match = re.search(r'@([A-Za-z0-9.-]+)', email)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None  # Return None if there's no valid domain\n",
    "\n",
    "# Apply the extract_domain function to the 'publisher' column\n",
    "df['domain'] = df['publisher'].apply(extract_domain)\n",
    "\n",
    "# Step 2: Count occurrences of each domain\n",
    "domain_counts = df['domain'].value_counts()\n",
    "\n",
    "# Step 3: Visualize the top domains (top 10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "domain_counts.head(10).plot(kind='bar', color='b')\n",
    "plt.title('Top 10 Domains Contributing Most to the News Feed')\n",
    "plt.xlabel('Domain (Organization)')\n",
    "plt.ylabel('Number of Articles Published')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print the most frequent domains\n",
    "print(\"\\nTop Domains by Frequency:\")\n",
    "print(domain_counts.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
