{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Alignmnent \n",
    "## normalize dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Date      Open      High       Low     Close  \\\n",
      "0 1980-12-12 00:00:00+00:00  0.128348  0.128906  0.128348  0.128348   \n",
      "1 1980-12-15 00:00:00+00:00  0.122210  0.122210  0.121652  0.121652   \n",
      "2 1980-12-16 00:00:00+00:00  0.113281  0.113281  0.112723  0.112723   \n",
      "3 1980-12-17 00:00:00+00:00  0.115513  0.116071  0.115513  0.115513   \n",
      "4 1980-12-18 00:00:00+00:00  0.118862  0.119420  0.118862  0.118862   \n",
      "\n",
      "   Adj Close     Volume  Dividends  Stock Splits  Unnamed: 0  \\\n",
      "0   0.098943  469033600        0.0           0.0      357064   \n",
      "1   0.093781  175884800        0.0           0.0      357064   \n",
      "2   0.086898  105728000        0.0           0.0      357064   \n",
      "3   0.089049   86441600        0.0           0.0      357064   \n",
      "4   0.091630   73449600        0.0           0.0      357064   \n",
      "\n",
      "                                            headline  \\\n",
      "0  ETFs To Watch April 28, 2011 (DGP, IEO, PRN, IDX)   \n",
      "1  ETFs To Watch April 28, 2011 (DGP, IEO, PRN, IDX)   \n",
      "2  ETFs To Watch April 28, 2011 (DGP, IEO, PRN, IDX)   \n",
      "3  ETFs To Watch April 28, 2011 (DGP, IEO, PRN, IDX)   \n",
      "4  ETFs To Watch April 28, 2011 (DGP, IEO, PRN, IDX)   \n",
      "\n",
      "                                                 url      publisher  \\\n",
      "0  https://www.benzinga.com/etfs/bond-etfs/11/04/...  ETF Professor   \n",
      "1  https://www.benzinga.com/etfs/bond-etfs/11/04/...  ETF Professor   \n",
      "2  https://www.benzinga.com/etfs/bond-etfs/11/04/...  ETF Professor   \n",
      "3  https://www.benzinga.com/etfs/bond-etfs/11/04/...  ETF Professor   \n",
      "4  https://www.benzinga.com/etfs/bond-etfs/11/04/...  ETF Professor   \n",
      "\n",
      "                       date stock  \n",
      "0 2011-04-28 01:01:48+00:00   DGP  \n",
      "1 2011-04-28 01:01:48+00:00   DGP  \n",
      "2 2011-04-28 01:01:48+00:00   DGP  \n",
      "3 2011-04-28 01:01:48+00:00   DGP  \n",
      "4 2011-04-28 01:01:48+00:00   DGP  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load news dataset (analyst ratings)\n",
    "news_df = pd.read_csv('../assets/data/raw_analyst_ratings.csv')\n",
    "\n",
    "# Load stock price datasets for each stock\n",
    "stock_dfs = {\n",
    "    'AAPL': pd.read_csv('../assets/data/AAPL_historical_data.csv'),\n",
    "    'AMZN': pd.read_csv('../assets/data/AMZN_historical_data.csv'),\n",
    "    'GOOG': pd.read_csv('../assets/data/GOOG_historical_data.csv'),\n",
    "    'MSFT': pd.read_csv('../assets/data/MSFT_historical_data.csv'),\n",
    "    'META': pd.read_csv('../assets/data/META_historical_data.csv'),\n",
    "    'NVDA': pd.read_csv('../assets/data/NVDA_historical_data.csv'),\n",
    "    'TSLA': pd.read_csv('../assets/data/TSLA_historical_data.csv')\n",
    "}\n",
    "\n",
    "# Normalize timestamps for news data (ensure both date and time are captured)\n",
    "news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaT in the 'date' column\n",
    "news_df = news_df.dropna(subset=['date'])\n",
    "\n",
    "# Normalize timestamps for stock price data (ensure both date and time are captured)\n",
    "for stock, stock_data in stock_dfs.items():\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'], errors='coerce')\n",
    "\n",
    "# Convert stock Date columns to timezone-aware (UTC) to match news data\n",
    "for stock, stock_data in stock_dfs.items():\n",
    "    if stock_data['Date'].dt.tz is None:  # Check if timezone is naive\n",
    "        stock_data['Date'] = stock_data['Date'].dt.tz_localize('UTC')  # Localize to UTC\n",
    "    else:\n",
    "        stock_data['Date'] = stock_data['Date'].dt.tz_convert('UTC')  # Convert to UTC if already aware\n",
    "\n",
    "# Convert news date column to UTC timezone (if it's not already)\n",
    "if news_df['date'].dt.tz is None:  # Check if timezone is naive\n",
    "    news_df['date'] = news_df['date'].dt.tz_localize('UTC')  # Localize to UTC\n",
    "else:\n",
    "    news_df['date'] = news_df['date'].dt.tz_convert('UTC')  # Convert to UTC if already aware\n",
    "\n",
    "# Align news with stock prices\n",
    "aligned_dfs = {}\n",
    "\n",
    "for stock, stock_data in stock_dfs.items():\n",
    "    # Merge the stock data with news data based on the nearest date\n",
    "    aligned_news = pd.merge_asof(\n",
    "        stock_data.sort_values('Date'),\n",
    "        news_df.sort_values('date'),\n",
    "        left_on='Date',\n",
    "        right_on='date',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    aligned_dfs[stock] = aligned_news\n",
    "\n",
    "# Handle missing data by dropping rows with missing values after the merge\n",
    "for stock in aligned_dfs:\n",
    "    aligned_dfs[stock] = aligned_dfs[stock].dropna()\n",
    "\n",
    "# Example: Check the first few rows of the aligned data for AAPL\n",
    "aapl_aligned_data = aligned_dfs['AAPL']\n",
    "print(aapl_aligned_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Results:\n",
      "sentiment\n",
      "Neutral     934914\n",
      "Positive    341178\n",
      "Negative    131236\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Load data from a CSV file\n",
    "file_path = \"../assets/data/raw_analyst_ratings.csv\"  # Replace with the actual path to your file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset has a 'headline' column\n",
    "if 'headline' not in df.columns:\n",
    "    raise ValueError(\"The dataset must contain a 'headline' column.\")\n",
    "\n",
    "# Perform sentiment analysis\n",
    "def analyze_sentiment(headline):\n",
    "    analysis = TextBlob(headline)\n",
    "    polarity = analysis.polarity  # Sentiment polarity: -1 (negative) to +1 (positive)\n",
    "    if polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Apply the sentiment analysis function to the headlines\n",
    "df['sentiment'] = df['headline'].apply(analyze_sentiment)\n",
    "\n",
    "# Count the number of headlines by sentiment\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "\n",
    "# Print results\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "print(sentiment_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Stock Returns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Close  Daily_Return\n",
      "Date                              \n",
      "1980-12-12  0.128348           NaN\n",
      "1980-12-15  0.121652     -5.217061\n",
      "1980-12-16  0.112723     -7.339788\n",
      "1980-12-17  0.115513      2.475091\n",
      "1980-12-18  0.118862      2.899246\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of stock filenames\n",
    "stock_files = {\n",
    "    'AAPL': '../assets/data/AAPL_historical_data.csv',\n",
    "    'AMZN': '../assets/data/AMZN_historical_data.csv',\n",
    "    'GOOG': '../assets/data/GOOG_historical_data.csv',\n",
    "    'MSFT': '../assets/data/MSFT_historical_data.csv',\n",
    "    'META': '../assets/data/META_historical_data.csv',\n",
    "    'NVDA': '../assets/data/NVDA_historical_data.csv',\n",
    "    'TSLA': '../assets/data/TSLA_historical_data.csv'\n",
    "}\n",
    "\n",
    "# Dictionary to store daily returns for each stock\n",
    "daily_returns = {}\n",
    "\n",
    "# Loop through each stock and calculate the daily returns\n",
    "for stock, file_path in stock_files.items():\n",
    "    # Load stock data\n",
    "    stock_data = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure the 'Date' column is in datetime format\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "\n",
    "    # Set 'Date' as the index\n",
    "    stock_data.set_index('Date', inplace=True)\n",
    "\n",
    "    # Calculate daily returns based on the 'Close' price (percentage change)\n",
    "    stock_data['Daily_Return'] = stock_data['Close'].pct_change() * 100\n",
    "\n",
    "    # Store the daily returns in the dictionary\n",
    "    daily_returns[stock] = stock_data[['Close', 'Daily_Return']]\n",
    "\n",
    "    # Optionally, save the results to a new CSV file\n",
    "    stock_data.to_csv(f'../assets/data/{stock}_with_daily_returns.csv')\n",
    "\n",
    "# Example: Print the first few rows of the daily returns for AAPL\n",
    "print(daily_returns['AAPL'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Sentiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'headline', 'url', 'publisher', 'date', 'stock'], dtype='object')\n",
      "                       date  sentiment_score\n",
      "0 2011-04-28 01:01:48+00:00         0.000000\n",
      "1 2011-04-28 17:49:29+00:00         0.136364\n",
      "2 2011-04-28 19:00:36+00:00         0.000000\n",
      "3 2011-04-29 17:47:06+00:00        -0.166667\n",
      "4 2011-04-29 20:11:05+00:00         0.500000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sample news data with 'date' and 'sentiment_score' columns\n",
    "news_data = pd.read_csv('../assets/data/raw_analyst_ratings.csv')\n",
    "\n",
    "# Ensure the 'date' column is in datetime format\n",
    "news_data['date'] = pd.to_datetime(news_data['date'], errors='coerce')\n",
    "\n",
    "print(news_data.columns) \n",
    "\n",
    "# Function to calculate sentiment polarity using TextBlob\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(str(text))  # Ensure text is in string format\n",
    "    return blob.sentiment.polarity  # You can also use blob.sentiment.subjectivity if needed\n",
    "\n",
    "# Calculate sentiment scores for each article\n",
    "news_data['sentiment_score'] = news_data['headline'].apply(get_sentiment)\n",
    "\n",
    "# Check if timezone is naive and localize or convert to UTC\n",
    "if news_data['date'].dt.tz is None:  # Check if timezone is naive\n",
    "    news_data['date'] = news_data['date'].dt.tz_localize('UTC')  # Localize to UTC\n",
    "else:\n",
    "    news_data['date'] = news_data['date'].dt.tz_convert('UTC')\n",
    "\n",
    "# Calculate daily average sentiment scores by grouping by 'date'\n",
    "daily_sentiment = news_data.groupby('date')['sentiment_score'].mean().reset_index()\n",
    "\n",
    "# Display the first few rows of the aggregated sentiment scores\n",
    "print(daily_sentiment.head())\n",
    "\n",
    "# Optionally, save the aggregated data to a CSV file\n",
    "daily_sentiment.to_csv('../assets/data/daily_sentiment_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
